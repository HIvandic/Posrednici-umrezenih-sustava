{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c6211a",
   "metadata": {},
   "source": [
    "# <center>POSREDNICI UMREŽENIH SUSTAVA<center>\n",
    "<center>Ak. god. 2021./2022.<center>\n",
    "\n",
    "    \n",
    "## <center>2. laboratorijska vježba: Apache Spark<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707967c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5ef096",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PUS Lab 2.\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb0edc",
   "metadata": {},
   "source": [
    "### Jednostavan primjer pipeline-a i učenja modela logističke regresije na klasifikaciji teksta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce545fde",
   "metadata": {},
   "source": [
    "1) Uvezivanje potrebnih komponenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1283157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95396b",
   "metadata": {},
   "source": [
    "2) Kreacija DataFrame objekta za učenje i testiranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2138f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([(0, \"a b c d e spark\", 1.0),(1, \"b d\", 0.0),(2, \"spark f g h\", 1.0),(3, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([(4, \"spark i j k\"),(5, \"l m n\"),(6, \"spark hadoop spark\"),(7, \"apache hadoop\")], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1acb20d",
   "metadata": {},
   "source": [
    "3) Definicija pipeline-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d49dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate sentences to words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# Convert set of words to vectors (fixed len)\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(),\n",
    "outputCol=\"features\")\n",
    "\n",
    "# Set model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# Pipline object \n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461bfd1",
   "metadata": {},
   "source": [
    "4) Učenje modela na podacima za učenje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c078c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8b0a7",
   "metadata": {},
   "source": [
    "5) Izvršavanje predikcija na testnom skupu i njihov ispis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1dd8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.6292098489668476,0.3707901510331524], prediction=0.000000\n",
      "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.13412348342566055,0.8658765165743394], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21916146",
   "metadata": {},
   "source": [
    "### Učenje modela za klasifikaciju sentimenta osvrta na filmJednostavan primjer pipeline-a i učenja modela logističke regresije na klasifikaciji teksta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de722f60",
   "metadata": {},
   "source": [
    "1. Preuzimanje skupa pod, predprocesiranje, pretvorba u mala slova, izrada dataframea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2855c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e394c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return re.sub(r'[^A-Za-z0-9 ]+', \"\", data.lower())\n",
    "\n",
    "def load(path, test):\n",
    "    data = []\n",
    "    labels = []\n",
    "    file_id = 0 \n",
    "    \n",
    "    cur_path = path + \"pos/\"\n",
    "    for file in os.listdir(cur_path):\n",
    "        f = open(cur_path + file, \"r\", encoding=\"utf8\")\n",
    "        whole = f.readlines()\n",
    "        text = preprocess(whole[0])\n",
    "        if test:\n",
    "            data.append((file_id, text))\n",
    "            labels.append(1.0)\n",
    "        else:\n",
    "            data.append((file_id, text, 1.0))\n",
    "        file_id += 1\n",
    "        \n",
    "    cur_path = path + \"neg/\"\n",
    "    for file in os.listdir(cur_path):\n",
    "        f = open(cur_path + file, \"r\", encoding=\"utf8\")\n",
    "        whole = f.readlines()\n",
    "        text = preprocess(whole[0])\n",
    "        if test:\n",
    "            data.append((file_id, text))\n",
    "            labels.append(0.0)\n",
    "        else:\n",
    "            data.append((file_id, text, 0.0))\n",
    "        file_id += 1\n",
    "        \n",
    "    if test:\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0bc43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load(\"D:/datasetPus/train/\", False)\n",
    "test, labels = load(\"D:/datasetPus/test/\", True)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_train = sc.parallelize(train, numSlices=100)\n",
    "training = rdd_train.toDF([\"id\", \"text\", \"label\"])\n",
    "rdd_test = sc.parallelize(test, numSlices=100)\n",
    "test = rdd_test.toDF([\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b374b24",
   "metadata": {},
   "source": [
    "2. Izrada pipelinea i vektorizacija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0e3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dbb09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"no_stopwords\") #, stopWords=StopWordsRemover.loadDefaultStopWords('english'))\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"features\", vocabSize=1000)\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e31c10",
   "metadata": {},
   "source": [
    "3. Učenje i evaluacija modela logističke regresije (točnost, preciznost i odziv), broj FN i FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470eb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d44213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(metrics):\n",
    "    print(\"Accuracy (točnost) = \", metrics.accuracy)\n",
    "    print(\"Precision (preciznost) = \", metrics.precision(1.0))\n",
    "    print(\"Recall (odziv) = \", metrics.recall(1.0))\n",
    "    print(\"Confusion matrix (matrica konfuzije) = \\n\", metrics.confusionMatrix().toArray())\n",
    "    # TP FP\n",
    "    # FN TN\n",
    "\n",
    "def evaluate(model, test, labels):\n",
    "    prediction = model.transform(test)\n",
    "    selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "    \n",
    "    data = []\n",
    "    for row in selected.collect():\n",
    "        file_id, text, prediction = row\n",
    "        data.append((prediction, labels[file_id]))\n",
    "        \n",
    "    predictionAndLabels = sc.parallelize(data)\n",
    "                                         \n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    get_values(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab11fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (točnost) =  0.8542\n",
      "Precision (preciznost) =  0.8460877042132416\n",
      "Recall (odziv) =  0.86592\n",
      "Confusion matrix (matrica konfuzije) = \n",
      " [[10531.  1969.]\n",
      " [ 1676. 10824.]]\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(training)\n",
    "evaluate(model, test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7128c8",
   "metadata": {},
   "source": [
    "4. 2-grami kao ulaz u model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa93abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ec84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (točnost) =  0.74324\n",
      "Precision (preciznost) =  0.725238906585673\n",
      "Recall (odziv) =  0.7832\n",
      "Confusion matrix (matrica konfuzije) = \n",
      " [[8791. 3709.]\n",
      " [2710. 9790.]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"no_stopwords\")\n",
    "ngram = NGram(n=2, inputCol=remover.getOutputCol(), outputCol=\"ngrams\")\n",
    "cv = CountVectorizer(inputCol=ngram.getOutputCol(), outputCol=\"features\", vocabSize=1000)\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, ngram, cv, lr])\n",
    "\n",
    "model = pipeline.fit(training)\n",
    "evaluate(model, test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68d50f",
   "metadata": {},
   "source": [
    "5. Spajanje vektora značajki jednorječne vektorizacije i 2-gram vektorizacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a040b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (točnost) =  0.85432\n",
      "Precision (preciznost) =  0.8489599747872676\n",
      "Recall (odziv) =  0.862\n",
      "Confusion matrix (matrica konfuzije) = \n",
      " [[10583.  1917.]\n",
      " [ 1725. 10775.]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"no_stopwords\")\n",
    "ngram = NGram(n=2, inputCol=remover.getOutputCol(), outputCol=\"ngrams\")\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"vectors\", vocabSize=1000)\n",
    "cv_n = CountVectorizer(inputCol=ngram.getOutputCol(), outputCol=\"vectors_n\", vocabSize=1000)\n",
    "vecassembler = VectorAssembler(inputCols=[cv.getOutputCol(), cv_n.getOutputCol()], outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, ngram, cv, cv_n, vecassembler, lr])\n",
    "\n",
    "model = pipeline.fit(training)\n",
    "evaluate(model, test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15784a1",
   "metadata": {},
   "source": [
    "6. SVM i naivni Bayesov umjesto logističke regresije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00f05683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40bb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"no_stopwords\")\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"features\", vocabSize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec2d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SVM\n",
      "Accuracy (točnost) =  0.85656\n",
      "Precision (preciznost) =  0.8443294190358467\n",
      "Recall (odziv) =  0.87432\n",
      "Confusion matrix (matrica konfuzije) = \n",
      " [[10485.  2015.]\n",
      " [ 1571. 10929.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"1. SVM\")\n",
    "svm = LinearSVC(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, svm])\n",
    "model = pipeline.fit(training)\n",
    "evaluate(model, test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7117e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Naive Bayes\n",
      "Accuracy (točnost) =  0.828\n",
      "Precision (preciznost) =  0.8274760383386581\n",
      "Recall (odziv) =  0.8288\n",
      "Confusion matrix (matrica konfuzije) = \n",
      " [[10340.  2160.]\n",
      " [ 2140. 10360.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Naive Bayes\")\n",
    "nb = NaiveBayes(smoothing=1.0, modelType='multinomial')\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, nb])\n",
    "model = pipeline.fit(training)\n",
    "evaluate(model, test, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
